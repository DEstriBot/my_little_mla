{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "b86336ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Regression:\n",
    "    def __init__(self, training_mode='GD', lr=0.01, max_iter=1000, eps=0.00001, yps=0.9):\n",
    "        self.lr = lr\n",
    "        self._n = max_iter\n",
    "        self.eps = eps\n",
    "        self.training_mode = training_mode\n",
    "        self.yps = yps\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # defining a loss function\n",
    "        self.define_cost()\n",
    "        \n",
    "        self.X = np.column_stack([X, np.ones((X.shape[0]))])\n",
    "        self.y = y\n",
    "        \n",
    "        self.coef_ = np.random.normal(size=(self.X.shape[1]), scale=0.2)\n",
    "        \n",
    "        self._train()\n",
    "        \n",
    "    def _train(self):\n",
    "        \"\"\"\n",
    "        Здесь:\n",
    "        1) Выбираем алгоритм оптимизации\n",
    "        2) Запускаем алгоритм (без цикла, цикл внутри алгоритма)\n",
    "        3) Сохраняем в селфовые переменные коеф, интерсепт\n",
    "        \"\"\"\n",
    "        _modes = {\n",
    "            'GD' : self._gradient_descent,\n",
    "            'RMSProp' : self._RMSProp,\n",
    "            'Nesterov' : self._nesterov\n",
    "        }\n",
    "        \n",
    "        self.coef_, self.intercept_ = _modes[self.training_mode](self.coef_)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.predict_func(X)\n",
    "    \n",
    "    def _gradient_descent(self, coef):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(self._n):\n",
    "            old_coef = coef\n",
    "            coef = old_coef - self.lr * self.cost_func(self.X, self.y, old_coef)\n",
    "\n",
    "            if np.linalg.norm(old_coef - coef, ord=2) <= self.eps:\n",
    "                break\n",
    "\n",
    "        return (coef[:-1], coef[-1])\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return self.score_func(X, y)\n",
    "        \n",
    "    def _nesterov(self, coef):\n",
    "        Vt = np.zeros(self.X.shape[1])\n",
    "\n",
    "        for i in range(self._n):\n",
    "            old_coef = coef\n",
    "            \n",
    "            impulse = old_coef - (self.yps * Vt)\n",
    "            Vt = (self.yps * Vt) + (self.lr * self.cost_func(self.X, self.y, old_coef))\n",
    "            coef = old_coef - Vt\n",
    "\n",
    "            if np.linalg.norm(old_coef - coef, ord=2) <= self.eps:\n",
    "                break\n",
    "        \n",
    "        return (coef[:-1], coef[-1])\n",
    "        \n",
    "    def _RMSProp(self, coef):\n",
    "        Egt = 0\n",
    "\n",
    "        for i in range(self._n):\n",
    "            old_coef = coef\n",
    "            \n",
    "            g = self.cost_func(self.X, self.y, old_coef)\n",
    "            Egt = (self.yps * Egt) + (1 - self.yps) * g**2\n",
    "            coef = old_coef - (self.lr / np.sqrt(Egt + 1e-30)) * g\n",
    "            \n",
    "            if np.linalg.norm(old_coef - coef, ord=2) <= self.eps:\n",
    "                break\n",
    "                \n",
    "        return (coef[:-1], coef[-1])\n",
    "    \n",
    "class LinearRegression(Regression):\n",
    "    def define_cost(self):\n",
    "        \n",
    "        def grad_mse(X, y, w):\n",
    "            yproba = X @ w\n",
    "            return 2/len(X)*(y - yproba) @ (-X)\n",
    "\n",
    "        self.cost_func = grad_mse\n",
    "        \n",
    "    def predict_func(self, X):\n",
    "        return X @ self.coef_ + self.intercept_\n",
    "    \n",
    "    def score_func(self, X, y):\n",
    "        yproba = X @ self.coef_\n",
    "        return np.average((y - yproba) ** 2)\n",
    "    \n",
    "    \n",
    "class LogisticRegression(Regression):\n",
    "    def define_cost(self):\n",
    "        \n",
    "        def grad_logloss(X, y, w):\n",
    "            yproba = self.sigmoid(X @ w)\n",
    "            return X.T @ (yproba - y)\n",
    "        \n",
    "        self.cost_func = grad_logloss\n",
    "    \n",
    "    def predict_func(self, X):\n",
    "        yproba = self.sigmoid(X @ self.coef_)\n",
    "        yhat = np.where(yproba >= 0.5, 1, 0)\n",
    "        return yhat\n",
    "    \n",
    "    def score_func(self, X, y):\n",
    "        yproba = self.sigmoid(X @ self.coef_)\n",
    "        return -np.sum(y * np.log(yproba + 1e-30) + (1 - y) * np.log(1 - yproba + 1e-30))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x.astype(float)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
